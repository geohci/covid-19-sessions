{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://notebook1003.eqiad.wmnet:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Jupyter Pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbd0fccc940>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SET spark.sql.shuffle.partitions = 1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "from random import randint, choice\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dataset of Covid-19-related pages\n",
    "* Note: this is based on: https://covid-data.wmflabs.org/pagesNoHumans\n",
    "* Additional preprocessing is done to remove duplicates, non-wiki, and non-namespace 1 articles.\n",
    "* The following articles (in all languages) are also added: [Coronavirus](https://en.wikipedia.org/wiki/Coronavirus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSV: \"pagesPerProjectNonHumans20200330.tsv\"\n",
    "# Pickle: \"pagesPerProjectNonHumans20200330.pickle\"\n",
    "# Parquet: \"/user/dsaez/pagesRelatedWithCOVID19-upto20200301.parquet\"\n",
    "pages_path = \"pagesPerProjectNonHumans20200420_pluscoronavirus.tsv\"\n",
    "covid_table_name = 'covid_pages'\n",
    "if pages_path.endswith('.parquet'):\n",
    "    spark.read.parquet(pages_path).createOrReplaceTempView(covid_table_name)\n",
    "elif pages_path.endswith('.pickle'):\n",
    "    print(\"Current Pandas version: {0} -- probably not compatible with file and can't be upgraded per: \"\n",
    "          \"https://wikitech.wikimedia.org/wiki/SWAP#Spark\".format(pd.__version__))\n",
    "    df = pd.read_pickle(pages_path)\n",
    "    spark.createDataFrame(df).createOrReplaceTempView(covid_table_name)\n",
    "elif pages_path.endswith('.tsv'):\n",
    "    df = pd.read_csv(pages_path, sep='\\t')\n",
    "    df['page_id'] = df['pageid']\n",
    "    df = df[['project', 'page_id']]\n",
    "    spark.createDataFrame(df).createOrReplaceTempView(covid_table_name)\n",
    "else:\n",
    "    print(\"Did not recognize file-type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "| project|   string|   null|\n",
      "| page_id|   bigint|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe {0}'.format(covid_table_name)).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------+\n",
      "|          page_title|     project|           P31-Label|  pageid|\n",
      "+--------------------+------------+--------------------+--------+\n",
      "|2019–20 coronavir...|en.wikipedia|{'public health e...|62750956|\n",
      "|2019–20 coronavir...|en.wikipedia|{'Wikimedia list ...|62938755|\n",
      "|2019–20 coronavir...|en.wikipedia|{'disease outbrea...|63039926|\n",
      "|2019–20 coronavir...|en.wikipedia|{'disease outbreak'}|63183247|\n",
      "|2019–20 coronavir...|en.wikipedia|{'disease outbreak'}|63325727|\n",
      "|2019–20 coronavir...|en.wikipedia|{'disease outbreak'}|63004998|\n",
      "|Coronavirus disea...|en.wikipedia|{'emerging infect...|63030231|\n",
      "|List of deaths du...|en.wikipedia|{'Wikimedia list ...|63417935|\n",
      "|List of events af...|en.wikipedia|{'Wikimedia list ...|63351852|\n",
      "|Mental health dur...|en.wikipedia|              {None}|63499429|\n",
      "|Shortages related...|en.wikipedia|              {None}|63453597|\n",
      "|Timeline of the 2...|en.wikipedia|{'Wikimedia timel...|63052529|\n",
      "|Travel restrictio...|en.wikipedia|{'disease outbreak'}|63262736|\n",
      "|World Health Orga...|en.wikipedia|              {None}|63612693|\n",
      "+--------------------+------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: I haven't rerun this since I removed the page_title and P31-Label columns\n",
    "spark.sql('SELECT * FROM {0} WHERE page_title LIKE \"%2019%\" AND project = \"en.wikipedia\"'.format(covid_table_name)).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             project|count(1)|\n",
      "+--------------------+--------+\n",
      "|        en.wikipedia|     367|\n",
      "|        ar.wikipedia|     250|\n",
      "|        pt.wikipedia|     184|\n",
      "|        zh.wikipedia|     184|\n",
      "|        de.wikipedia|     175|\n",
      "|        ko.wikipedia|     161|\n",
      "|        vi.wikipedia|     146|\n",
      "|        uk.wikipedia|     117|\n",
      "|        fr.wikipedia|     112|\n",
      "|        he.wikipedia|      96|\n",
      "|        es.wikipedia|      82|\n",
      "|        tr.wikipedia|      82|\n",
      "|        ru.wikipedia|      70|\n",
      "|        ca.wikipedia|      60|\n",
      "|        pl.wikipedia|      59|\n",
      "|        nl.wikipedia|      50|\n",
      "|        id.wikipedia|      47|\n",
      "|        be.wikipedia|      43|\n",
      "|        uz.wikipedia|      38|\n",
      "|        ms.wikipedia|      38|\n",
      "|        pa.wikipedia|      38|\n",
      "|        et.wikipedia|      38|\n",
      "|        it.wikipedia|      37|\n",
      "|    zh-yue.wikipedia|      37|\n",
      "|        ja.wikipedia|      35|\n",
      "|        ht.wikipedia|      34|\n",
      "|        az.wikipedia|      33|\n",
      "|        ta.wikipedia|      32|\n",
      "|        ro.wikipedia|      31|\n",
      "|        fa.wikipedia|      31|\n",
      "|        ur.wikipedia|      27|\n",
      "|        th.wikipedia|      26|\n",
      "|        cs.wikipedia|      23|\n",
      "|        sq.wikipedia|      22|\n",
      "|        bn.wikipedia|      20|\n",
      "|        bg.wikipedia|      18|\n",
      "|        hi.wikipedia|      14|\n",
      "|        fi.wikipedia|      14|\n",
      "|        hu.wikipedia|      13|\n",
      "|        ka.wikipedia|      13|\n",
      "|        el.wikipedia|      13|\n",
      "|        sd.wikipedia|      13|\n",
      "|        sr.wikipedia|      12|\n",
      "|        kn.wikipedia|      12|\n",
      "|        sv.wikipedia|      11|\n",
      "|        lv.wikipedia|      11|\n",
      "|        gl.wikipedia|      11|\n",
      "|        no.wikipedia|      11|\n",
      "|        eu.wikipedia|      11|\n",
      "|        ml.wikipedia|      10|\n",
      "|       min.wikipedia|      10|\n",
      "|        bs.wikipedia|       9|\n",
      "|        si.wikipedia|       9|\n",
      "|       kab.wikipedia|       9|\n",
      "|        ug.wikipedia|       8|\n",
      "|        hy.wikipedia|       8|\n",
      "|        su.wikipedia|       8|\n",
      "|        ga.wikipedia|       7|\n",
      "|        sl.wikipedia|       7|\n",
      "|       arz.wikipedia|       7|\n",
      "|        mk.wikipedia|       7|\n",
      "|        tl.wikipedia|       7|\n",
      "|        cy.wikipedia|       7|\n",
      "|    simple.wikipedia|       7|\n",
      "|        jv.wikipedia|       7|\n",
      "|        eo.wikipedia|       6|\n",
      "| be-tarask.wikipedia|       6|\n",
      "|        bh.wikipedia|       6|\n",
      "|        oc.wikipedia|       6|\n",
      "|       pap.wikipedia|       6|\n",
      "|        sh.wikipedia|       6|\n",
      "|        da.wikipedia|       6|\n",
      "|        my.wikipedia|       6|\n",
      "|        te.wikipedia|       5|\n",
      "|        kk.wikipedia|       5|\n",
      "|       wuu.wikipedia|       5|\n",
      "|       als.wikipedia|       5|\n",
      "|        hr.wikipedia|       5|\n",
      "|       sco.wikipedia|       4|\n",
      "|       mnw.wikipedia|       4|\n",
      "|        sk.wikipedia|       4|\n",
      "|        sw.wikipedia|       4|\n",
      "|        la.wikipedia|       4|\n",
      "|       ckb.wikipedia|       4|\n",
      "|        lb.wikipedia|       4|\n",
      "|        lt.wikipedia|       4|\n",
      "|       ast.wikipedia|       4|\n",
      "|        yo.wikipedia|       4|\n",
      "|        br.wikipedia|       4|\n",
      "|        qu.wikipedia|       4|\n",
      "|       lmo.wikipedia|       4|\n",
      "|        is.wikipedia|       4|\n",
      "|        gu.wikipedia|       3|\n",
      "|        am.wikipedia|       3|\n",
      "|        an.wikipedia|       3|\n",
      "|       tcy.wikipedia|       3|\n",
      "|       azb.wikipedia|       3|\n",
      "|        cv.wikipedia|       3|\n",
      "|        as.wikipedia|       3|\n",
      "|zh-min-nan.wikipedia|       3|\n",
      "+--------------------+--------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_per_db = spark.sql('SELECT project, count(*) FROM {0} GROUP BY project'.format(covid_table_name))\n",
    "count_per_db.sort('count(1)', ascending=False).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate table with anonymized/filtered webrequests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_execute = True\n",
    "table_name = 'isaacj.covid19_sessions'\n",
    "create_table_query = \"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {0} (\n",
    "        session_hash     STRING  COMMENT 'Unique identifier for session',\n",
    "        continent        STRING  COMMENT 'Reader continent per IP geolocation',\n",
    "        country          STRING  COMMENT 'Reader country per IP geolocation',\n",
    "        subdivision      STRING  COMMENT 'Reader subdivision per IP geolocation',\n",
    "        timezone         STRING  COMMENT 'Reader timezone per IP geolocation',\n",
    "        hour             INT     COMMENT 'Unpadded hour of request',\n",
    "        project          STRING  COMMENT 'Wikipedia project -- e.g., en.wikipedia',\n",
    "        namespace_id     INT     COMMENT '0 for articles; 1 for talk pages',\n",
    "        qid              STRING  COMMENT 'Wikidata ID associated with article -- e.g., Q42',\n",
    "        page_id          INT     COMMENT 'Page ID for article viewed -- automatically resolves redirects',\n",
    "        title            STRING  COMMENT 'Page title viewed -- preserves redirects taken',\n",
    "        is_covid         BOOLEAN COMMENT '1 if matches Covid-19 list; 0 otherwise',\n",
    "        referer          STRING  COMMENT 'Host of referer URL -- e.g., www.google.com',\n",
    "        referer_class    STRING  COMMENT 'High-level class of referer (external, search engine, direct, internal)',\n",
    "        access_method    STRING  COMMENT 'Version of Wikipedia viewed -- desktop or mobile',\n",
    "        last_access      STRING  COMMENT 'Date device was last seen per WMF-Last-Access cookie',\n",
    "        min_btw_pvs      DOUBLE  COMMENT 'Integer # of minutes since prior pageview in session',\n",
    "        session_sequence INT     COMMENT 'Index of pageview in session -- starting at 1 and ordered chronologically'\n",
    "    )\n",
    "    PARTITIONED BY (\n",
    "        year             INT     COMMENT 'Unpadded year of request',\n",
    "        month            INT     COMMENT 'Unpadded month of request',\n",
    "        day              INT     COMMENT 'Unpadded day of request')\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 'hdfs://analytics-hadoop/user/isaacj/covid19'\n",
    "    \"\"\".format(table_name)\n",
    "\n",
    "if do_execute:\n",
    "    spark.sql(create_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Table\n",
    "Filtered down to:\n",
    "* Wikipedia project family\n",
    "* Pageviews\n",
    "* Namespace = 0 (articles) and Namespace = 1 (talk pages)\n",
    "* No sessions with evidence of edit activity\n",
    "* Agents labeled as users (NOTE: this is imperfect so users w/ >500 pageviews are also removed)\n",
    "\n",
    "Anonymization steps:\n",
    "* Hash user-agent/IP\n",
    "* Only retain geographic information at the subdivision level (no cities, latitude, or longitude)\n",
    "* Blacklist certain small or sensitive countries\n",
    "* Only retain a sample of users to reduce likelihood that an individual will appear across multiple days in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To query from 2020-01-08 up to but not including 2020-01-20\n"
     ]
    }
   ],
   "source": [
    "first_date = \"2020-01-01\"\n",
    "num_days = 90\n",
    "session_length = 60 * 60  # 1 hour = 60 sec * 60 min\n",
    "\n",
    "dt = datetime.strptime(first_date, \"%Y-%m-%d\")\n",
    "print(\"To query from {0} up to but not including {1}\".format(datetime.strftime(dt, \"%Y-%m-%d\"),\n",
    "                                                             datetime.strftime(dt + timedelta(days=num_days), \"%Y-%m-%d\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    WITH wikipedia_projects AS (\n",
      "        SELECT DISTINCT hostname,\n",
      "               dbname\n",
      "          FROM wmf_raw.mediawiki_project_namespace_map\n",
      "         WHERE snapshot = '2020-01'\n",
      "               AND hostname LIKE '%wikipedia%'\n",
      "        ),\n",
      "    exploded_wikidata_links AS (\n",
      "        SELECT wiki_db,\n",
      "               page_id,\n",
      "               FIRST_VALUE(item_id, true) OVER (PARTITION BY wiki_db, page_id ORDER BY snapshot DESC) as item_id\n",
      "          FROM wmf.wikidata_item_page_link\n",
      "         WHERE snapshot >= '2020-01-06'\n",
      "               AND page_namespace = 0\n",
      "        ),\n",
      "    wikidata_ids AS (\n",
      "        SELECT DISTINCT SUBSTR(p.hostname, 0, length(p.hostname)-4) AS project,\n",
      "               wd.page_id AS page_id,\n",
      "               wd.item_id AS item_id\n",
      "          FROM exploded_wikidata_links wd\n",
      "         INNER JOIN wikipedia_projects p\n",
      "               ON (wd.wiki_db = p.dbname)\n",
      "        ),\n",
      "    blacklisted_countries AS (\n",
      "        SELECT DISTINCT country\n",
      "          FROM wmf.geoeditors_blacklist_country\n",
      "         WHERE country NOT IN ('Russia', 'Iran')\n",
      "        ),\n",
      "    users_to_keep AS (\n",
      "        SELECT sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256) AS user,\n",
      "               w.geocoded_data['country'] AS country,\n",
      "               MAX(CAST(COALESCE(cvd.page_id, -1) > 0 AS int)) AS is_covid,\n",
      "               COUNT(1) AS num_pviews\n",
      "          FROM wmf.webrequest w\n",
      "          LEFT JOIN covid_pages cvd\n",
      "               ON (w.page_id = cvd.page_id AND pageview_info['project'] = cvd.project)\n",
      "          LEFT ANTI JOIN blacklisted_countries bc\n",
      "               ON (w.geocoded_data['country'] = country)\n",
      "         WHERE webrequest_source = 'text'\n",
      "               AND normalized_host.project_family = 'wikipedia'\n",
      "               AND year = 2020 AND month = 1 AND day = 8\n",
      "               AND agent_type = 'user'\n",
      "               AND access_method <> 'mobile app'\n",
      "               AND ((is_pageview AND (namespace_id = 0 OR namespace_id = 1))\n",
      "                    OR (uri_query LIKE '%action=edit%' OR uri_query LIKE '%action=visualeditor%' OR uri_query LIKE '%&intestactions=edit&intestactionsdetail=full&uiprop=options%'))\n",
      "             GROUP BY sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256), w.geocoded_data['country']\n",
      "            HAVING COUNT(1) < 500\n",
      "                   AND MAX(CAST(NOT is_pageview as int)) = 0\n",
      "                   AND ((is_covid > 0 AND CONV(SUBSTR(user, 49), 16, 10) / 18446744073709551615 < 0.5)\n",
      "                        OR CONV(SUBSTR(user, 49), 16, 10) / 18446744073709551615 < 0.01)\n",
      "        ),\n",
      "    countries_to_drop AS (\n",
      "           SELECT DISTINCT(country)\n",
      "             FROM (\n",
      "               SELECT country\n",
      "                 FROM users_to_keep\n",
      "                WHERE is_covid = 0\n",
      "                GROUP BY country\n",
      "               HAVING COUNT(DISTINCT(user)) < 500\n",
      "                UNION ALL\n",
      "               SELECT country\n",
      "                 FROM users_to_keep\n",
      "                GROUP BY country\n",
      "               HAVING SUM(num_pviews * is_covid) > (0.90 * SUM(num_pviews))\n",
      "                  ) c\n",
      "        ),\n",
      "    covid_to_drop AS (\n",
      "           SELECT country\n",
      "             FROM users_to_keep\n",
      "            WHERE is_covid = 1\n",
      "            GROUP BY country\n",
      "           HAVING COUNT(DISTINCT(user)) < 500\n",
      "        ),\n",
      "       sessions AS (\n",
      "           SELECT sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256) AS user,\n",
      "                  geocoded_data['continent'] AS continent,\n",
      "                  geocoded_data['country'] AS country,\n",
      "                  geocoded_data['subdivision'] AS subdivision,\n",
      "                  geocoded_data['timezone'] AS timezone,\n",
      "                  ts, hour,\n",
      "                  pageview_info['project'] as project,\n",
      "                  w.namespace_id as namespace_id,\n",
      "                  COALESCE(wd.item_id, 'None') as qid,\n",
      "                  w.page_id AS page_id,\n",
      "                  pageview_info['page_title'] AS title,\n",
      "                  CAST(COALESCE(cvd.page_id, -1) > 0 AS int) > 0 AS is_covid,\n",
      "                  PARSE_URL(referer, 'HOST') as referer,\n",
      "                  referer_class,\n",
      "                  access_method,\n",
      "                  COALESCE(x_analytics_map['WMF-Last-Access'], 'NULL') AS last_access,\n",
      "                  CASE\n",
      "                    WHEN (unix_timestamp(ts) - COALESCE(LAG(unix_timestamp(ts)) OVER (PARTITION BY sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256) ORDER BY ts), 0)) >= 3600\n",
      "                      THEN 1 ELSE 0\n",
      "                  END AS new_session\n",
      "             FROM wmf.webrequest w\n",
      "            INNER JOIN users_to_keep u\n",
      "                  ON (u.user = sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256))\n",
      "             LEFT ANTI JOIN countries_to_drop c\n",
      "                  ON (c.country = w.geocoded_data['country'])\n",
      "             LEFT ANTI JOIN covid_to_drop d\n",
      "                  ON (d.country = w.geocoded_data['country']\n",
      "                      AND u.is_covid = 1\n",
      "                      AND CONV(SUBSTR(sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256), 49), 16, 10) / 18446744073709551615 >= 0.01)\n",
      "             LEFT JOIN covid_pages cvd\n",
      "                  ON (w.pageview_info['project'] = cvd.project AND w.page_id = cvd.page_id)\n",
      "             LEFT JOIN wikidata_ids wd\n",
      "                  ON (w.pageview_info['project'] = wd.project AND w.page_id = wd.page_id)\n",
      "            WHERE webrequest_source = 'text'\n",
      "                  AND normalized_host.project_family = 'wikipedia'\n",
      "                  AND user_agent IS NOT NULL AND client_ip IS NOT NULL\n",
      "                  AND year = 2020 AND month = 1 AND day = 8\n",
      "                  AND agent_type = 'user'\n",
      "                  AND access_method <> 'mobile app'\n",
      "                  AND is_pageview\n",
      "                  AND (namespace_id = 0 OR namespace_id = 1)\n",
      "        ),\n",
      "      anonymized_sessions AS (\n",
      "          SELECT sha2(CONCAT(user, '<SALT-2>', SUM(new_session) OVER (PARTITION BY user ORDER BY ts)), 256) AS session_hash,\n",
      "                 continent, country, subdivision, timezone,\n",
      "                 ts, hour,\n",
      "                 project, qid, namespace_id, page_id, title, is_covid,\n",
      "                 referer, referer_class, access_method, last_access\n",
      "            FROM sessions s\n",
      "        )         \n",
      "       INSERT OVERWRITE TABLE isaacj.covid19_sessions\n",
      "       PARTITION(year=2020, month=1, day=8)\n",
      "       SELECT session_hash,\n",
      "              continent, country, subdivision, timezone,\n",
      "              hour,\n",
      "              project, qid, namespace_id, page_id, title, is_covid,\n",
      "              referer, referer_class, access_method, last_access,\n",
      "              ROW_NUMBER() OVER w as session_sequence,\n",
      "              ROUND(COALESCE(unix_timestamp(ts) - LAG(unix_timestamp(ts)) OVER w, 0) / 60) as min_btw_pvs\n",
      "         FROM anonymized_sessions\n",
      "       WINDOW w AS (PARTITION BY session_hash ORDER BY ts)\n",
      "     \n",
      "\n",
      "    WITH wikipedia_projects AS (\n",
      "        SELECT DISTINCT hostname,\n",
      "               dbname\n",
      "          FROM wmf_raw.mediawiki_project_namespace_map\n",
      "         WHERE snapshot = '2020-01'\n",
      "               AND hostname LIKE '%wikipedia%'\n",
      "        ),\n",
      "    exploded_wikidata_links AS (\n",
      "        SELECT wiki_db,\n",
      "               page_id,\n",
      "               FIRST_VALUE(item_id, true) OVER (PARTITION BY wiki_db, page_id ORDER BY snapshot DESC) as item_id\n",
      "          FROM wmf.wikidata_item_page_link\n",
      "         WHERE snapshot >= '2020-01-06'\n",
      "               AND page_namespace = 0\n",
      "        ),\n",
      "    wikidata_ids AS (\n",
      "        SELECT DISTINCT SUBSTR(p.hostname, 0, length(p.hostname)-4) AS project,\n",
      "               wd.page_id AS page_id,\n",
      "               wd.item_id AS item_id\n",
      "          FROM exploded_wikidata_links wd\n",
      "         INNER JOIN wikipedia_projects p\n",
      "               ON (wd.wiki_db = p.dbname)\n",
      "        ),\n",
      "    blacklisted_countries AS (\n",
      "        SELECT DISTINCT country\n",
      "          FROM wmf.geoeditors_blacklist_country\n",
      "         WHERE country NOT IN ('Russia', 'Iran')\n",
      "        ),\n",
      "    users_to_keep AS (\n",
      "        SELECT sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256) AS user,\n",
      "               w.geocoded_data['country'] AS country,\n",
      "               MAX(CAST(COALESCE(cvd.page_id, -1) > 0 AS int)) AS is_covid,\n",
      "               COUNT(1) AS num_pviews\n",
      "          FROM wmf.webrequest w\n",
      "          LEFT JOIN covid_pages cvd\n",
      "               ON (w.page_id = cvd.page_id AND pageview_info['project'] = cvd.project)\n",
      "          LEFT ANTI JOIN blacklisted_countries bc\n",
      "               ON (w.geocoded_data['country'] = country)\n",
      "         WHERE webrequest_source = 'text'\n",
      "               AND normalized_host.project_family = 'wikipedia'\n",
      "               AND year = 2020 AND month = 1 AND day = 9\n",
      "               AND agent_type = 'user'\n",
      "               AND access_method <> 'mobile app'\n",
      "               AND ((is_pageview AND (namespace_id = 0 OR namespace_id = 1))\n",
      "                    OR (uri_query LIKE '%action=edit%' OR uri_query LIKE '%action=visualeditor%' OR uri_query LIKE '%&intestactions=edit&intestactionsdetail=full&uiprop=options%'))\n",
      "             GROUP BY sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256), w.geocoded_data['country']\n",
      "            HAVING COUNT(1) < 500\n",
      "                   AND MAX(CAST(NOT is_pageview as int)) = 0\n",
      "                   AND ((is_covid > 0 AND CONV(SUBSTR(user, 49), 16, 10) / 18446744073709551615 < 0.5)\n",
      "                        OR CONV(SUBSTR(user, 49), 16, 10) / 18446744073709551615 < 0.01)\n",
      "        ),\n",
      "    countries_to_drop AS (\n",
      "           SELECT DISTINCT(country)\n",
      "             FROM (\n",
      "               SELECT country\n",
      "                 FROM users_to_keep\n",
      "                WHERE is_covid = 0\n",
      "                GROUP BY country\n",
      "               HAVING COUNT(DISTINCT(user)) < 500\n",
      "                UNION ALL\n",
      "               SELECT country\n",
      "                 FROM users_to_keep\n",
      "                GROUP BY country\n",
      "               HAVING SUM(num_pviews * is_covid) > (0.90 * SUM(num_pviews))\n",
      "                  ) c\n",
      "        ),\n",
      "    covid_to_drop AS (\n",
      "           SELECT country\n",
      "             FROM users_to_keep\n",
      "            WHERE is_covid = 1\n",
      "            GROUP BY country\n",
      "           HAVING COUNT(DISTINCT(user)) < 500\n",
      "        ),\n",
      "       sessions AS (\n",
      "           SELECT sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256) AS user,\n",
      "                  geocoded_data['continent'] AS continent,\n",
      "                  geocoded_data['country'] AS country,\n",
      "                  geocoded_data['subdivision'] AS subdivision,\n",
      "                  geocoded_data['timezone'] AS timezone,\n",
      "                  ts, hour,\n",
      "                  pageview_info['project'] as project,\n",
      "                  w.namespace_id as namespace_id,\n",
      "                  COALESCE(wd.item_id, 'None') as qid,\n",
      "                  w.page_id AS page_id,\n",
      "                  pageview_info['page_title'] AS title,\n",
      "                  CAST(COALESCE(cvd.page_id, -1) > 0 AS int) > 0 AS is_covid,\n",
      "                  PARSE_URL(referer, 'HOST') as referer,\n",
      "                  referer_class,\n",
      "                  access_method,\n",
      "                  COALESCE(x_analytics_map['WMF-Last-Access'], 'NULL') AS last_access,\n",
      "                  CASE\n",
      "                    WHEN (unix_timestamp(ts) - COALESCE(LAG(unix_timestamp(ts)) OVER (PARTITION BY sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256) ORDER BY ts), 0)) >= 3600\n",
      "                      THEN 1 ELSE 0\n",
      "                  END AS new_session\n",
      "             FROM wmf.webrequest w\n",
      "            INNER JOIN users_to_keep u\n",
      "                  ON (u.user = sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256))\n",
      "             LEFT ANTI JOIN countries_to_drop c\n",
      "                  ON (c.country = w.geocoded_data['country'])\n",
      "             LEFT ANTI JOIN covid_to_drop d\n",
      "                  ON (d.country = w.geocoded_data['country']\n",
      "                      AND u.is_covid = 1\n",
      "                      AND CONV(SUBSTR(sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256), 49), 16, 10) / 18446744073709551615 >= 0.01)\n",
      "             LEFT JOIN covid_pages cvd\n",
      "                  ON (w.pageview_info['project'] = cvd.project AND w.page_id = cvd.page_id)\n",
      "             LEFT JOIN wikidata_ids wd\n",
      "                  ON (w.pageview_info['project'] = wd.project AND w.page_id = wd.page_id)\n",
      "            WHERE webrequest_source = 'text'\n",
      "                  AND normalized_host.project_family = 'wikipedia'\n",
      "                  AND user_agent IS NOT NULL AND client_ip IS NOT NULL\n",
      "                  AND year = 2020 AND month = 1 AND day = 9\n",
      "                  AND agent_type = 'user'\n",
      "                  AND access_method <> 'mobile app'\n",
      "                  AND is_pageview\n",
      "                  AND (namespace_id = 0 OR namespace_id = 1)\n",
      "        ),\n",
      "      anonymized_sessions AS (\n",
      "          SELECT sha2(CONCAT(user, '<SALT-2>', SUM(new_session) OVER (PARTITION BY user ORDER BY ts)), 256) AS session_hash,\n",
      "                 continent, country, subdivision, timezone,\n",
      "                 ts, hour,\n",
      "                 project, qid, namespace_id, page_id, title, is_covid,\n",
      "                 referer, referer_class, access_method, last_access\n",
      "            FROM sessions s\n",
      "        )         \n",
      "       INSERT OVERWRITE TABLE isaacj.covid19_sessions\n",
      "       PARTITION(year=2020, month=1, day=9)\n",
      "       SELECT session_hash,\n",
      "              continent, country, subdivision, timezone,\n",
      "              hour,\n",
      "              project, qid, namespace_id, page_id, title, is_covid,\n",
      "              referer, referer_class, access_method, last_access,\n",
      "              ROW_NUMBER() OVER w as session_sequence,\n",
      "              ROUND(COALESCE(unix_timestamp(ts) - LAG(unix_timestamp(ts)) OVER w, 0) / 60) as min_btw_pvs\n",
      "         FROM anonymized_sessions\n",
      "       WINDOW w AS (PARTITION BY session_hash ORDER BY ts)\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "# Acceptance criteria:\n",
    "# * Control sessions: take 1% sample and retain if >= 500 unique userhashes\n",
    "# * Covid-19 sessions: take 50% sample and retain if:\n",
    "# ** control sample met threshold AND\n",
    "# ** covid-19 sample also meets 500 unique userhashes threshold\n",
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "for i in range(num_days):    \n",
    "    # salts for UA/IP hash (1st = userhash for whole day; 2nd = session hashes)\n",
    "    salt_one = ''.join(choice(string.ascii_lowercase + string.ascii_uppercase + string.digits) for _ in range(randint(8,16)))\n",
    "    salt_two = ''.join(choice(string.ascii_lowercase + string.ascii_uppercase + string.digits) for _ in range(randint(8,16)))\n",
    "\n",
    "    # matching by last digit of IP is ~random means of sampling devices (no obvious skew)\n",
    "    # Due to IPv6, IP addresses can actually end in [0-9] or [a-f]\n",
    "    # There are still many more IPv4 addresses than IPv6 though, so using a [0-9] gives you ~5x more data than a letter\n",
    "    # North America has a slightly higher proportion of IPv6 so they are undersampled slightly through this method\n",
    "    # This is not currently used in the full query but is useful for testing\n",
    "    # and can be added to WHERE clauses when selecting from wmf.webrequest: AND SUBSTR(ip, -1, 1) = {7}\n",
    "    ip_match = randint(0, 9)\n",
    "\n",
    "    # we keep each day separate and use new salts / ip_matches because:\n",
    "    #   * this keeps the processing simpler (e.g., for order-by clauses)\n",
    "    #   * this helps to preserve anonymization by limiting the amount of data associated with a single user ID\n",
    "    #   * a IP/UA hash has only been shown to captures most of the pageviews by an individual (~75% per former analyses)\n",
    "    #     for a given 24 hour period. Beyond 24 hours, it is unclear how effective this method is.\n",
    "    #   * limiting to 24-hours per hash should help ensure that we do not incorrectly merge multiple users together\n",
    "\n",
    "    # query for a given day\n",
    "    query_date = dt + timedelta(days=i)\n",
    "\n",
    "    # We exclude edits by the following logic:\n",
    "    # (uri_query LIKE '%action=edit%'): desktop wikitext editor\n",
    "    # (uri_query LIKE '%action=visualeditor%'): desktop and mobile visualeditor\n",
    "    # (uri_query LIKE '%&intestactions=edit&intestactionsdetail=full&uiprop=options%'): mobile wikitext editor\n",
    "    # NOTE for mobile wikitext editor: this is actually an API call so theoretically anyone could make it, but \n",
    "    # in practice we see literally 100% of calls to intestactions=edit are mobile wikitext editor and the rest\n",
    "    # of the URL parameters in the string is to ensure no false positives.\n",
    "    \n",
    "    # We also exclude mobile_app users because they are:\n",
    "    # * a small proportion of our pageviews\n",
    "    # * more identifiable as a result (and have unique app IDs in x_analytics_header)\n",
    "    # * I have not figured out what their edit clauses look like\n",
    "    \n",
    "    # We exclude a blacklist of countries (wmf.geoeditors_blacklist_country)\n",
    "    # with the exception of Iran and Russia given their research relevance for Covid-19\n",
    "    \n",
    "    # We exclude users with > 500 pageviews in a day because anecdotally these are split between:\n",
    "    # * serious power users\n",
    "    # * a bunch of different people all using the same proxy (e.g., Google Weblight)\n",
    "    # * unidentified bots\n",
    "    \n",
    "    # We take 1% of all users and 50% of users who viewed a Covid-19-related page in their session:\n",
    "    # we randomly sample individual users from a day's worth of data to reduce the likelihood of any one\n",
    "    # person showing up consistently in each day's sample. This is done by taking a userhash (before sessionization)\n",
    "    # and converting it to decimal and then sampling based on the final digits of that resulting number.\n",
    "    # For example, a sha256 userhash might be '0d03031932aa1d96d54327ea4754b393a62feb61585fef6cf06daee9cf848b27'\n",
    "    # For sampling, if we take the final 16 characters of that userhash ('f06daee9cf848b27')\n",
    "    #   and convert that to decimal, it is: 17324695660796349223\n",
    "    #   if we then divide this by 18446744073709551615, we will get a number between 0 and 1\n",
    "    #   in this case it is 0.9391736336542796\n",
    "    #   we can simply define our sampling by only retaining users for which that number X is below Y\n",
    "    #   e.g., a 1% sample is X < 0.01 and a 50% sample would be X < 0.5 -- in both cases, this user would not be retained\n",
    "    # Alternatively (for sampling): \n",
    "    #   In decimal, the example userhash would be: 5885388957350101327411052618704501086212173454334169283944732915681982712615\n",
    "    #   If we take the final four digits of that (2615), we can determine whether the user is part of a 1% sample by\n",
    "    #   testing whether that number is less than 100. In this case, no, but if the userhashes are randomly distributed\n",
    "    #   (which they should be), then 1% of the final four digits should be between 0000 and 0099 and the other 99% should\n",
    "    #   be from 0100 to 9999. This can be adjusted to e.g., 50% by checking whether the digits are less than 5000.\n",
    "    \n",
    "    # After all the pageviews associated with the sampled users are collected, we order them by timestamp and\n",
    "    # separate them into 1 or more sessions, where a session is defined as >1 hour between consecutive pageviews.\n",
    "    # Userhashes are then reassigned so it's not easy to determine whether a given user had multiple sessions in\n",
    "    # a day or not.\n",
    "    \n",
    "    # Timestamps in the final dataset are reported only as year, month, day, hour. To preserve the research value\n",
    "    # of knowing time between pageviews while still reducing the privacy risk that fine-grained timestamps pose,\n",
    "    # we also compute the number of minutes (integer) between each pageview and indicate the order of pageviews.\n",
    "    \n",
    "    query = \"\"\"\n",
    "    WITH wikipedia_projects AS (\n",
    "        SELECT DISTINCT hostname,\n",
    "               dbname\n",
    "          FROM wmf_raw.mediawiki_project_namespace_map\n",
    "         WHERE snapshot = '2020-01'\n",
    "               AND hostname LIKE '%wikipedia%'\n",
    "        ),\n",
    "    exploded_wikidata_links AS (\n",
    "        SELECT wiki_db,\n",
    "               page_id,\n",
    "               FIRST_VALUE(item_id, true) OVER (PARTITION BY wiki_db, page_id ORDER BY snapshot DESC) as item_id\n",
    "          FROM wmf.wikidata_item_page_link\n",
    "         WHERE snapshot >= '2020-01-06'\n",
    "               AND page_namespace = 0\n",
    "        ),\n",
    "    wikidata_ids AS (\n",
    "        SELECT DISTINCT SUBSTR(p.hostname, 0, length(p.hostname)-4) AS project,\n",
    "               wd.page_id AS page_id,\n",
    "               wd.item_id AS item_id\n",
    "          FROM exploded_wikidata_links wd\n",
    "         INNER JOIN wikipedia_projects p\n",
    "               ON (wd.wiki_db = p.dbname)\n",
    "        ),\n",
    "    blacklisted_countries AS (\n",
    "        SELECT DISTINCT country\n",
    "          FROM wmf.geoeditors_blacklist_country\n",
    "         WHERE country NOT IN ('Russia', 'Iran')\n",
    "        ),\n",
    "    users_to_keep AS (\n",
    "        SELECT sha2(CONCAT(user_agent, client_ip, '{4}'), 256) AS user,\n",
    "               w.geocoded_data['country'] AS country,\n",
    "               MAX(CAST(COALESCE(cvd.page_id, -1) > 0 AS int)) AS is_covid,\n",
    "               COUNT(1) AS num_pviews\n",
    "          FROM wmf.webrequest w\n",
    "          LEFT JOIN covid_pages cvd\n",
    "               ON (w.page_id = cvd.page_id AND pageview_info['project'] = cvd.project)\n",
    "          LEFT ANTI JOIN blacklisted_countries bc\n",
    "               ON (w.geocoded_data['country'] = country)\n",
    "         WHERE webrequest_source = 'text'\n",
    "               AND normalized_host.project_family = 'wikipedia'\n",
    "               AND year = {1} AND month = {2} AND day = {3}\n",
    "               AND agent_type = 'user'\n",
    "               AND access_method <> 'mobile app'\n",
    "               AND ((is_pageview AND (namespace_id = 0 OR namespace_id = 1))\n",
    "                    OR (uri_query LIKE '%action=edit%' OR uri_query LIKE '%action=visualeditor%' OR uri_query LIKE '%&intestactions=edit&intestactionsdetail=full&uiprop=options%'))\n",
    "             GROUP BY sha2(CONCAT(user_agent, client_ip, '{4}'), 256), w.geocoded_data['country']\n",
    "            HAVING COUNT(1) < 500\n",
    "                   AND MAX(CAST(NOT is_pageview as int)) = 0\n",
    "                   AND ((is_covid > 0 AND CONV(SUBSTR(user, 49), 16, 10) / 18446744073709551615 < 0.5)\n",
    "                        OR CONV(SUBSTR(user, 49), 16, 10) / 18446744073709551615 < 0.01)\n",
    "        ),\n",
    "    countries_to_drop AS (\n",
    "           SELECT DISTINCT(country)\n",
    "             FROM (\n",
    "               SELECT country\n",
    "                 FROM users_to_keep\n",
    "                WHERE is_covid = 0\n",
    "                GROUP BY country\n",
    "               HAVING COUNT(DISTINCT(user)) < 500\n",
    "                UNION ALL\n",
    "               SELECT country\n",
    "                 FROM users_to_keep\n",
    "                GROUP BY country\n",
    "               HAVING SUM(num_pviews * is_covid) > (0.90 * SUM(num_pviews))\n",
    "                  ) c\n",
    "        ),\n",
    "    covid_to_drop AS (\n",
    "           SELECT country\n",
    "             FROM users_to_keep\n",
    "            WHERE is_covid = 1\n",
    "            GROUP BY country\n",
    "           HAVING COUNT(DISTINCT(user)) < 500\n",
    "        ),\n",
    "       sessions AS (\n",
    "           SELECT sha2(CONCAT(user_agent, client_ip, '{4}'), 256) AS user,\n",
    "                  geocoded_data['continent'] AS continent,\n",
    "                  geocoded_data['country'] AS country,\n",
    "                  geocoded_data['subdivision'] AS subdivision,\n",
    "                  geocoded_data['timezone'] AS timezone,\n",
    "                  ts, hour,\n",
    "                  pageview_info['project'] as project,\n",
    "                  w.namespace_id as namespace_id,\n",
    "                  COALESCE(wd.item_id, 'None') as qid,\n",
    "                  w.page_id AS page_id,\n",
    "                  pageview_info['page_title'] AS title,\n",
    "                  CAST(COALESCE(cvd.page_id, -1) > 0 AS int) > 0 AS is_covid,\n",
    "                  PARSE_URL(referer, 'HOST') as referer,\n",
    "                  referer_class,\n",
    "                  access_method,\n",
    "                  COALESCE(x_analytics_map['WMF-Last-Access'], 'NULL') AS last_access,\n",
    "                  CASE\n",
    "                    WHEN (unix_timestamp(ts) - COALESCE(LAG(unix_timestamp(ts)) OVER (PARTITION BY sha2(CONCAT(user_agent, client_ip, '{4}'), 256) ORDER BY ts), 0)) >= {6}\n",
    "                      THEN 1 ELSE 0\n",
    "                  END AS new_session\n",
    "             FROM wmf.webrequest w\n",
    "            INNER JOIN users_to_keep u\n",
    "                  ON (u.user = sha2(CONCAT(user_agent, client_ip, '{4}'), 256))\n",
    "             LEFT ANTI JOIN countries_to_drop c\n",
    "                  ON (c.country = w.geocoded_data['country'])\n",
    "             LEFT ANTI JOIN covid_to_drop d\n",
    "                  ON (d.country = w.geocoded_data['country']\n",
    "                      AND u.is_covid = 1\n",
    "                      AND CONV(SUBSTR(sha2(CONCAT(user_agent, client_ip, '{4}'), 256), 49), 16, 10) / 18446744073709551615 >= 0.01)\n",
    "             LEFT JOIN covid_pages cvd\n",
    "                  ON (w.pageview_info['project'] = cvd.project AND w.page_id = cvd.page_id)\n",
    "             LEFT JOIN wikidata_ids wd\n",
    "                  ON (w.pageview_info['project'] = wd.project AND w.page_id = wd.page_id)\n",
    "            WHERE webrequest_source = 'text'\n",
    "                  AND normalized_host.project_family = 'wikipedia'\n",
    "                  AND user_agent IS NOT NULL AND client_ip IS NOT NULL\n",
    "                  AND year = {1} AND month = {2} AND day = {3}\n",
    "                  AND agent_type = 'user'\n",
    "                  AND access_method <> 'mobile app'\n",
    "                  AND is_pageview\n",
    "                  AND (namespace_id = 0 OR namespace_id = 1)\n",
    "        ),\n",
    "      anonymized_sessions AS (\n",
    "          SELECT sha2(CONCAT(user, '{5}', SUM(new_session) OVER (PARTITION BY user ORDER BY ts)), 256) AS session_hash,\n",
    "                 continent, country, subdivision, timezone,\n",
    "                 ts, hour,\n",
    "                 project, qid, namespace_id, page_id, title, is_covid,\n",
    "                 referer, referer_class, access_method, last_access\n",
    "            FROM sessions s\n",
    "        )         \n",
    "       INSERT OVERWRITE TABLE {0}\n",
    "       PARTITION(year={1}, month={2}, day={3})\n",
    "       SELECT session_hash,\n",
    "              continent, country, subdivision, timezone,\n",
    "              hour,\n",
    "              project, namespace_id, qid, page_id, title, is_covid,\n",
    "              referer, referer_class, access_method, last_access,\n",
    "              ROUND(COALESCE(unix_timestamp(ts) - LAG(unix_timestamp(ts)) OVER w, 0) / 60) as min_btw_pvs,\n",
    "              ROW_NUMBER() OVER w as session_sequence\n",
    "         FROM anonymized_sessions\n",
    "       WINDOW w AS (PARTITION BY session_hash ORDER BY ts)\n",
    "     \"\"\".format(table_name, query_date.year, query_date.month, query_date.day,\n",
    "                salt_one, salt_two, session_length, ip_match)    \n",
    "        \n",
    "    if print_for_hive:\n",
    "        print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "    else:\n",
    "        print(re.sub(r'SUBSTR\\(ip, -1, 1\\) = [0-9]',\n",
    "                     \"SUBSTR(ip, -1, 1) = <#>\",\n",
    "                     re.sub(r\"CONCAT\\(user_agent, client_ip, '[a-zA-Z0-9]+'\\)\",\n",
    "                            \"CONCAT(user_agent, client_ip, '<SALT-1>')\",\n",
    "                            re.sub(r\"CONCAT\\(user, '[a-zA-Z0-9]+', SUM\\(\",\n",
    "                                   \"CONCAT(user, '<SALT-2>', SUM(\",\n",
    "                                   query))))\n",
    "    \n",
    "    if do_execute:\n",
    "        spark.sql(query)\n",
    "\n",
    "# clear salt and ip_match so not accidentally retained\n",
    "ip_match = None\n",
    "salt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year = 2020\n",
    "month = 1\n",
    "day = 1\n",
    "spark.sql(\"\"\"\n",
    "    SELECT country,\n",
    "           SUM(is_covid) as covid_sessions,\n",
    "           COUNT(1) - SUM(is_covid) as non_covid_sessions,\n",
    "           SUM(num_pviews * is_covid) as covid_pviews,\n",
    "           SUM(num_pviews) - SUM(num_pviews * is_covid) as noncovid_pviews\n",
    "      FROM (SELECT session_hash,\n",
    "                   country,\n",
    "                   CAST(MAX(is_covid) as int) as is_covid,\n",
    "                   COUNT(1) as num_pviews\n",
    "              FROM {0}\n",
    "             WHERE year = {1} and month = {2} and day = {3}\n",
    "             GROUP BY session_hash, country\n",
    "           ) s\n",
    "     GROUP BY country\n",
    "     ORDER BY covid_pviews DESC\n",
    "     LIMIT 1000\n",
    "     \"\"\".format(table_name, year, month, day)).show(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if need to push the data to a TSV for further processing (not currently used)\n",
    "write_to_tsv = False\n",
    "if write_to_tsv:\n",
    "    for i in range(num_days):\n",
    "        query_date = dt + timedelta(days=i)\n",
    "\n",
    "        day_data = spark.sql('SELECT * FROM {0} WHERE year = {1} and month = {2} and day = {3}'.format(\n",
    "            table_name, query_date.year, query_date.month, query_date.day))\n",
    "        day_data.show(10)\n",
    "        day_data.coalesce(1).write.csv(path=\"/user/isaacj/covid19_tsvs/wr_{0}\".format(t_date), compression=\"gzip\", header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate intermediate data for a given country to inspect\n",
    "NOTE: this is for testing purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_for_hive = False\n",
    "do_execute = False\n",
    "country = 'Ghana'\n",
    "\n",
    "# salt for UA/IP hash\n",
    "salt = ''.join(choice(string.ascii_lowercase + string.ascii_uppercase + string.digits) for _ in range(randint(8,16)))\n",
    "\n",
    "# matching by last digit of IP is ~random means of sampling devices (no obvious skew)\n",
    "# Due to IPv6, IP addresses can actually end in [0-9] or [a-f]\n",
    "# There are still many more IPv4 addresses than IPv6 though, so using a [0-9] gives you ~5x more data than a letter\n",
    "# North America has a slightly higher proportion of IPv6 so they are undersampled slightly through this method\n",
    "# consider adding to innermost where clause: AND SUBSTR(ip, -1, 1) = {4}\n",
    "ip_match = randint(0, 9)\n",
    "        \n",
    "# query for a given day\n",
    "query_date = dt\n",
    "    \n",
    "query = \"\"\"\n",
    "    SELECT sha2(CONCAT(user_agent, client_ip, '{0}'), 256) AS user,\n",
    "           MAX(CAST(COALESCE(cvd.page_id, -1) > 0 AS int)) AS is_covid,\n",
    "           MAX(CAST(NOT is_pageview as int)) AS editor,\n",
    "           COUNT(1) AS num_pviews,\n",
    "           COUNT(1) < 500 AS power_user,\n",
    "           CONV(sha2(SUBSTR(CONCAT(user_agent, client_ip, '{0}'), 256), 49), 16, 10) / 18446744073709551615 as rand_sample\n",
    "      FROM wmf.webrequest w\n",
    "      LEFT JOIN covid_pages cvd\n",
    "           ON (w.page_id = cvd.page_id AND pageview_info['project'] = cvd.project)\n",
    "     WHERE webrequest_source = 'text'\n",
    "           AND normalized_host.project_family = 'wikipedia'\n",
    "           AND geocoded_data['country'] = '{5}'\n",
    "           AND year = {1} AND month = {2} AND day = {3}\n",
    "           AND agent_type = 'user'\n",
    "           AND access_method <> 'mobile app'\n",
    "           AND ((is_pageview AND (namespace_id = 0 OR namespace_id = 1))\n",
    "                OR (uri_query LIKE '%action=edit%' OR uri_query LIKE '%action=visualeditor%' OR uri_query LIKE '%&intestactions=edit&intestactionsdetail=full&uiprop=options%'))\n",
    "         GROUP BY sha2(CONCAT(user_agent, client_ip, '{0}'), 256)\n",
    " \"\"\".format(salt, query_date.year, query_date.month, query_date.day, ip_match, country)    \n",
    "\n",
    "table = 'isaacj.{0}_sessions_{1}'.format(country, datetime.strftime(query_date, \"%Y%m%d\"))\n",
    "query = 'CREATE TABLE {0} AS {1}'.format(table, query)\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(re.sub(r'SUBSTR\\(ip, -1, 1\\) = [0-9]', \"SUBSTR(ip, -1, 1) = <#>\",\n",
    "                 re.sub(r\"CONCAT\\(user_agent, client_ip, '[a-zA-Z0-9]+'\\)\", '<UH CLAUSE>', query)))\n",
    "\n",
    "if do_execute:\n",
    "    spark.sql(query)\n",
    "\n",
    "# clear salt and ip_match so not accidentally retained\n",
    "ip_match = None\n",
    "salt = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN",
   "language": "python",
   "name": "spark_yarn_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
