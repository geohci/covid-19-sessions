{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://notebook1003.eqiad.wmnet:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Jupyter Pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0befccd940>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SET spark.sql.shuffle.partitions = 1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "from random import randint, choice\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSV: \"pagesPerProjectNonHumans20200330.tsv\"\n",
    "# Pickle: \"pagesPerProjectNonHumans20200330.pickle\"\n",
    "# Parquet: \"/user/dsaez/pagesRelatedWithCOVID19-upto20200301.parquet\"\n",
    "pages_path = \"covid19_pages_20200405.tsv\"\n",
    "covid_table_name = 'covid_pages'\n",
    "if pages_path.endswith('.parquet'):\n",
    "    spark.read.parquet(pages_path).createOrReplaceTempView(covid_table_name)\n",
    "elif pages_path.endswith('.pickle'):\n",
    "    print(\"Current Pandas version: {0} -- probably not compatible with file and can't be upgraded per: \"\n",
    "          \"https://wikitech.wikimedia.org/wiki/SWAP#Spark\".format(pd.__version__))\n",
    "    df = pd.read_pickle(pages_path)\n",
    "    spark.createDataFrame(df).createOrReplaceTempView(covid_table_name)\n",
    "elif pages_path.endswith('.tsv'):\n",
    "    df = pd.read_csv(pages_path, sep='\\t')\n",
    "    spark.createDataFrame(df).createOrReplaceTempView(covid_table_name)\n",
    "else:\n",
    "    print(\"Did not recognize file-type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|page_title|   string|   null|\n",
      "|   project|   string|   null|\n",
      "|   page_id|   bigint|   null|\n",
      "| P31-Label|   string|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe {0}'.format(covid_table_name)).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------+--------------------+\n",
      "|          page_title|     project| page_id|           P31-Label|\n",
      "+--------------------+------------+--------+--------------------+\n",
      "|2019–20 coronavir...|en.wikipedia|62750956|{'public health e...|\n",
      "|2019–20 coronavir...|en.wikipedia|62938755|{'Wikimedia list ...|\n",
      "|2019–20 coronavir...|en.wikipedia|63039926|{'disease outbreak'}|\n",
      "|2019–20 coronavir...|en.wikipedia|63325727|{'disease outbreak'}|\n",
      "|2019–20 coronavir...|en.wikipedia|63004998|{'disease outbreak'}|\n",
      "|Coronavirus disea...|en.wikipedia|63030231|{'emerging infect...|\n",
      "|List of deaths fr...|en.wikipedia|63417935|{'Wikimedia list ...|\n",
      "|List of events af...|en.wikipedia|63351852|{'Wikimedia list ...|\n",
      "|Mental health dur...|en.wikipedia|63499429|               {nan}|\n",
      "|Shortages related...|en.wikipedia|63453597|               {nan}|\n",
      "|Timeline of the 2...|en.wikipedia|63052529|{'Wikimedia timel...|\n",
      "|Travel restrictio...|en.wikipedia|63262736|{'disease outbreak'}|\n",
      "+--------------------+------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM {0} WHERE page_title LIKE \"%2019%\" AND project = \"en.wikipedia\"'.format(covid_table_name)).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             project|count(1)|\n",
      "+--------------------+--------+\n",
      "|        en.wikipedia|     313|\n",
      "|        ar.wikipedia|     210|\n",
      "|        zh.wikipedia|     175|\n",
      "|        pt.wikipedia|     169|\n",
      "|        vi.wikipedia|     139|\n",
      "|        ko.wikipedia|     130|\n",
      "|        de.wikipedia|     106|\n",
      "|        fr.wikipedia|      97|\n",
      "|        uk.wikipedia|      94|\n",
      "|        es.wikipedia|      75|\n",
      "|        he.wikipedia|      72|\n",
      "|        tr.wikipedia|      71|\n",
      "|        ru.wikipedia|      58|\n",
      "|        pl.wikipedia|      56|\n",
      "|        ca.wikipedia|      55|\n",
      "|        nl.wikipedia|      44|\n",
      "|        id.wikipedia|      36|\n",
      "|        et.wikipedia|      36|\n",
      "|    zh-yue.wikipedia|      34|\n",
      "|        ms.wikipedia|      33|\n",
      "|        it.wikipedia|      33|\n",
      "|        be.wikipedia|      32|\n",
      "|        ht.wikipedia|      32|\n",
      "|        ja.wikipedia|      31|\n",
      "|        ro.wikipedia|      29|\n",
      "|        fa.wikipedia|      28|\n",
      "|        az.wikipedia|      25|\n",
      "|        ta.wikipedia|      24|\n",
      "|        ur.wikipedia|      22|\n",
      "|        cs.wikipedia|      22|\n",
      "|        th.wikipedia|      22|\n",
      "|        sq.wikipedia|      20|\n",
      "|        bg.wikipedia|      17|\n",
      "|        bn.wikipedia|      15|\n",
      "|        hu.wikipedia|      12|\n",
      "|        el.wikipedia|      12|\n",
      "|        sr.wikipedia|      11|\n",
      "|        pa.wikipedia|      11|\n",
      "|        ka.wikipedia|      11|\n",
      "|        fi.wikipedia|      10|\n",
      "|        sv.wikipedia|      10|\n",
      "|        sd.wikipedia|      10|\n",
      "|        gl.wikipedia|       9|\n",
      "|        no.wikipedia|       9|\n",
      "|       min.wikipedia|       8|\n",
      "|        eu.wikipedia|       8|\n",
      "|        jv.wikipedia|       7|\n",
      "|        tl.wikipedia|       7|\n",
      "|        hi.wikipedia|       7|\n",
      "|        si.wikipedia|       7|\n",
      "|        ml.wikipedia|       7|\n",
      "|        hy.wikipedia|       7|\n",
      "|        ug.wikipedia|       7|\n",
      "|        bs.wikipedia|       7|\n",
      "|       kab.wikipedia|       7|\n",
      "|        uz.wikipedia|       6|\n",
      "|        su.wikipedia|       6|\n",
      "|        lv.wikipedia|       6|\n",
      "|        my.wikipedia|       6|\n",
      "|    simple.wikipedia|       6|\n",
      "|        mk.wikipedia|       6|\n",
      "|        te.wikipedia|       5|\n",
      "|        oc.wikipedia|       5|\n",
      "|        sh.wikipedia|       5|\n",
      "|        bh.wikipedia|       5|\n",
      "|        eo.wikipedia|       5|\n",
      "|       arz.wikipedia|       5|\n",
      "|       als.wikipedia|       5|\n",
      "|        sl.wikipedia|       4|\n",
      "|        hr.wikipedia|       4|\n",
      "|        kk.wikipedia|       4|\n",
      "| be-tarask.wikipedia|       4|\n",
      "|        sk.wikipedia|       4|\n",
      "|        cy.wikipedia|       4|\n",
      "|        lb.wikipedia|       4|\n",
      "|        br.wikipedia|       4|\n",
      "|       wuu.wikipedia|       4|\n",
      "|        kn.wikipedia|       4|\n",
      "|       mnw.wikipedia|       4|\n",
      "|        da.wikipedia|       4|\n",
      "|zh-classical.wiki...|       3|\n",
      "|       cdo.wikipedia|       3|\n",
      "|        is.wikipedia|       3|\n",
      "|       pap.wikipedia|       3|\n",
      "|        lt.wikipedia|       3|\n",
      "|        cv.wikipedia|       3|\n",
      "|zh-min-nan.wikipedia|       3|\n",
      "|        la.wikipedia|       3|\n",
      "|       lmo.wikipedia|       3|\n",
      "|       sco.wikipedia|       3|\n",
      "|        sw.wikipedia|       3|\n",
      "|       ast.wikipedia|       3|\n",
      "|        qu.wikipedia|       3|\n",
      "|        an.wikipedia|       3|\n",
      "|       ckb.wikipedia|       3|\n",
      "|       hak.wikipedia|       3|\n",
      "|        as.wikipedia|       3|\n",
      "|       vec.wikipedia|       2|\n",
      "|        ha.wikipedia|       2|\n",
      "|       frr.wikipedia|       2|\n",
      "+--------------------+--------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_per_db = spark.sql('SELECT project, count(*) FROM {0} GROUP BY project'.format(covid_table_name))\n",
    "count_per_db.sort('count(1)', ascending=False).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To query from 2020-01-01 up to but not including 2020-03-31\n"
     ]
    }
   ],
   "source": [
    "first_date = \"2020-01-01\"\n",
    "num_days = 90\n",
    "session_length = 60 * 60  # 1 hour = 60 sec * 60 min\n",
    "table_name = 'isaacj.covid19_sessions'\n",
    "\n",
    "dt = datetime.strptime(first_date, \"%Y-%m-%d\")\n",
    "print(\"To query from {0} up to but not including {1}\".format(datetime.strftime(dt, \"%Y-%m-%d\"),\n",
    "                                                             datetime.strftime(dt + timedelta(days=num_days), \"%Y-%m-%d\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate table with anonymized/filtered webrequests\n",
    "\n",
    "Filtered down to:\n",
    "* Wikipedia project family\n",
    "* Pageviews\n",
    "* Namespace = 0 (articles) and Namespace = 1 (talk pages)\n",
    "* No sessions with evidence of edit activity\n",
    "* Agents labeled as users (NOTE: this is imperfect so users w/ >500 pageviews are also removed)\n",
    "\n",
    "Anonymization steps:\n",
    "* Hash user-agent/IP\n",
    "* Only retain geographic information at the subdivision level (no cities, latitude, or longitude)\n",
    "* Blacklist certain small or sensitive countries\n",
    "* Only retain a sample of users to reduce likelihood that an individual will appear across multiple days in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_execute = False\n",
    "create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {0} (\n",
    "        session_hash     STRING  COMMENT 'Unique identifier for session',\n",
    "        continent        STRING  COMMENT 'Reader continent per IP geolocation',\n",
    "        country          STRING  COMMENT 'Reader country per IP geolocation',\n",
    "        subdivision      STRING  COMMENT 'Reader subdivision per IP geolocation',\n",
    "        timezone         STRING  COMMENT 'Reader timezone per IP geolocation',\n",
    "        hour             INT     COMMENT 'Unpadded hour of request',\n",
    "        project          STRING  COMMENT 'Wikipedia project -- e.g., en.wikipedia',\n",
    "        namespace_id     INT     COMMENT '0 for articles; 1 for talk pages',\n",
    "        qid              STRING  COMMENT 'Wikidata ID associated with article -- e.g., Q42',\n",
    "        page_id          INT     COMMENT 'Page ID for article viewed -- automatically resolves redirects',\n",
    "        title            STRING  COMMENT 'Page title viewed -- preserves redirects taken',\n",
    "        is_covid         BOOLEAN COMMENT '1 if matches Covid-19 list; 0 otherwise',\n",
    "        referer          STRING  COMMENT 'Host of referer URL -- e.g., www.google.com',\n",
    "        referer_class    STRING  COMMENT 'High-level class of referer (external, search engine, direct, internal)',\n",
    "        access_method    STRING  COMMENT 'Version of Wikipedia viewed -- desktop or mobile',\n",
    "        last_access      STRING  COMMENT 'Date device was last seen per WMF-Last-Access cookie',\n",
    "        min_btw_pvs      DOUBLE  COMMENT 'Integer # of minutes since prior pageview in session',\n",
    "        session_sequence INT     COMMENT 'Index of pageview in session -- starting at 1 and ordered chronologically'\n",
    "    )\n",
    "    PARTITIONED BY (\n",
    "        year             INT     COMMENT 'Unpadded year of request',\n",
    "        month            INT     COMMENT 'Unpadded month of request',\n",
    "        day              INT     COMMENT 'Unpadded day of request')\n",
    "    STORED AS PARQUET\n",
    "    \"\"\".format(table_name)\n",
    "\n",
    "if do_execute:\n",
    "    spark.sql(create_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    INSERT OVERWRITE TABLE isaacj.covid19_sessions\n",
      "           PARTITION(year=2020, month=1, day=1)\n",
      "    WITH wikipedia_projects AS (\n",
      "        SELECT DISTINCT hostname,\n",
      "               dbname\n",
      "          FROM wmf_raw.mediawiki_project_namespace_map\n",
      "         WHERE snapshot = '2020-01'\n",
      "               AND hostname LIKE '%wikipedia%'\n",
      "        ),\n",
      "    wikidata_ids AS (\n",
      "        SELECT SUBSTR(p.hostname, 0, length(p.hostname)-4) AS project,\n",
      "               wd.page_id AS page_id,\n",
      "               wd.item_id AS item_id\n",
      "          FROM wmf.wikidata_item_page_link wd\n",
      "          LEFT JOIN wikipedia_projects p\n",
      "               ON (wd.wiki_db = p.dbname)\n",
      "         WHERE snapshot = '2020-03-02'\n",
      "               AND page_namespace = 0\n",
      "        ),\n",
      "    blacklisted_countries AS (\n",
      "        SELECT DISTINCT country\n",
      "          FROM wmf.geoeditors_blacklist_country\n",
      "         WHERE country NOT IN ('Russia', 'Iran')\n",
      "        ),\n",
      "    users_to_keep AS (\n",
      "        SELECT sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256) AS user,\n",
      "               w.geocoded_data['country'] AS country,\n",
      "               MAX(CAST(COALESCE(cvd.page_id, -1) > 0 AS int)) AS is_covid,\n",
      "               COUNT(1) AS num_pviews\n",
      "          FROM wmf.webrequest w\n",
      "          LEFT JOIN covid_pages cvd\n",
      "               ON (w.page_id = cvd.page_id AND pageview_info['project'] = cvd.project)\n",
      "          LEFT ANTI JOIN blacklisted_countries bc\n",
      "               ON (w.geocoded_data['country'] = country)\n",
      "         WHERE webrequest_source = 'text'\n",
      "               AND normalized_host.project_family = 'wikipedia'\n",
      "               AND year = 2020 AND month = 1 AND day = 1\n",
      "               AND agent_type = 'user'\n",
      "               AND access_method <> 'mobile app'\n",
      "               AND ((is_pageview AND (namespace_id = 0 OR namespace_id = 1))\n",
      "                    OR (uri_query LIKE '%action=edit%' OR uri_query LIKE '%action=visualeditor%' OR uri_query LIKE '%&intestactions=edit&intestactionsdetail=full&uiprop=options%'))\n",
      "             GROUP BY sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256), w.geocoded_data['country']\n",
      "            HAVING COUNT(1) < 500\n",
      "                   AND MAX(CAST(NOT is_pageview as int)) = 0\n",
      "                   AND ((is_covid > 0 AND CONV(SUBSTR(user, 49), 16, 10) / 18446744073709551615 < 0.5)\n",
      "                        OR CONV(SUBSTR(user, 49), 16, 10) / 18446744073709551615 < 0.01)\n",
      "        ),\n",
      "    countries_to_drop AS (\n",
      "           SELECT DISTINCT(country)\n",
      "             FROM (\n",
      "               SELECT country\n",
      "                 FROM users_to_keep\n",
      "                GROUP BY country, is_covid\n",
      "               HAVING COUNT(DISTINCT(user)) < 500\n",
      "                UNION ALL\n",
      "               SELECT country\n",
      "                 FROM users_to_keep\n",
      "                GROUP BY country\n",
      "               HAVING SUM(num_pviews * is_covid) > (0.90 * SUM(num_pviews))\n",
      "                  ) c\n",
      "        ),\n",
      "       sessions AS (\n",
      "           SELECT sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256) AS user,\n",
      "                  geocoded_data['continent'] AS continent,\n",
      "                  geocoded_data['country'] AS country,\n",
      "                  geocoded_data['subdivision'] AS subdivision,\n",
      "                  geocoded_data['timezone'] AS timezone,\n",
      "                  ts, hour,\n",
      "                  pageview_info['project'] as project,\n",
      "                  w.namespace_id as namespace_id,\n",
      "                  COALESCE(wd.item_id, 'None') as qid,\n",
      "                  w.page_id AS page_id,\n",
      "                  pageview_info['page_title'] AS title,\n",
      "                  CAST(COALESCE(cvd.page_id, -1) > 0 AS int) > 0 AS is_covid,\n",
      "                  PARSE_URL(referer, 'HOST') as referer,\n",
      "                  referer_class,\n",
      "                  access_method,\n",
      "                  COALESCE(x_analytics_map['WMF-Last-Access'], 'NULL') AS last_access,\n",
      "                  CASE\n",
      "                    WHEN (unix_timestamp(ts) - COALESCE(LAG(unix_timestamp(ts)) OVER (PARTITION BY sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256) ORDER BY ts), 0)) >= 3600\n",
      "                      THEN 1 ELSE 0\n",
      "                  END AS new_session\n",
      "             FROM wmf.webrequest w\n",
      "            INNER JOIN users_to_keep u\n",
      "                  ON (u.user = sha2(CONCAT(user_agent, client_ip, '<SALT-1>'), 256))\n",
      "             LEFT ANTI JOIN countries_to_drop c\n",
      "                  ON (c.country = w.geocoded_data['country'])\n",
      "             LEFT JOIN covid_pages cvd\n",
      "                  ON (w.pageview_info['project'] = cvd.project AND w.page_id = cvd.page_id)\n",
      "             LEFT JOIN wikidata_ids wd\n",
      "                  ON (w.pageview_info['project'] = wd.project AND w.page_id = wd.page_id)\n",
      "            WHERE webrequest_source = 'text'\n",
      "                  AND normalized_host.project_family = 'wikipedia'\n",
      "                  AND year = 2020 AND month = 1 AND day = 1\n",
      "                  AND agent_type = 'user'\n",
      "                  AND access_method <> 'mobile app'\n",
      "                  AND is_pageview\n",
      "                  AND (namespace_id = 0 OR namespace_id = 1)\n",
      "        ),\n",
      "      anonymized_sessions AS (\n",
      "          SELECT sha2(CONCAT(user, '<SALT-2>', SUM(new_session) OVER (PARTITION BY user ORDER BY ts)), 256) AS session_hash,\n",
      "                 continent, country, subdivision, timezone,\n",
      "                 year, ts,\n",
      "                 project, qid, namespace_id, page_id, title, is_covid,\n",
      "                 referer, referer_class, access_method, last_access\n",
      "            FROM sessions s\n",
      "        )           \n",
      "       SELECT session_hash,\n",
      "              continent, country, subdivision, timezone,\n",
      "              hour,\n",
      "              project, qid, namespace_id, page_id, title, is_covid,\n",
      "              referer, referer_class, access_method, last_access,\n",
      "              IF(sec_btw_pvs >= 3600, 0, ROUND(sec_btw_pvs / 60)) AS min_btw_pvs,\n",
      "              ROW_NUMBER() OVER w as session_sequence,\n",
      "              ROUND(COALESCE(unix_timestamp(ts) - LAG(unix_timestamp(ts)) OVER w, 0) / 60) as min_btw_pvs\n",
      "         FROM anonymized_sessions\n",
      "       WINDOW w AS (PARTITION BY session_hash ORDER BY ts)\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "for i in range(num_days):    \n",
    "    # salts for UA/IP hash (1st = userhash for whole day; 2nd = session hashes)\n",
    "    salt_one = ''.join(choice(string.ascii_lowercase + string.ascii_uppercase + string.digits) for _ in range(randint(8,16)))\n",
    "    salt_two = ''.join(choice(string.ascii_lowercase + string.ascii_uppercase + string.digits) for _ in range(randint(8,16)))\n",
    "\n",
    "    # matching by last digit of IP is ~random means of sampling devices (no obvious skew)\n",
    "    # Due to IPv6, IP addresses can actually end in [0-9] or [a-f]\n",
    "    # There are still many more IPv4 addresses than IPv6 though, so using a [0-9] gives you ~5x more data than a letter\n",
    "    # North America has a slightly higher proportion of IPv6 so they are undersampled slightly through this method\n",
    "    # This is not currently used in the full query but is useful for testing\n",
    "    # and can be added to WHERE clauses when selecting from wmf.webrequest: AND SUBSTR(ip, -1, 1) = {7}\n",
    "    ip_match = randint(0, 9)\n",
    "\n",
    "    # we keep each day separate and use new salts / ip_matches because:\n",
    "    #   * this keeps the processing simpler (e.g., for order-by clauses)\n",
    "    #   * this helps to preserve anonymization by limiting the amount of data associated with a single user ID\n",
    "    #   * a IP/UA hash has only been shown to captures most of the pageviews by an individual (~75% per former analyses)\n",
    "    #     for a given 24 hour period. Beyond 24 hours, it is unclear how effective this method is.\n",
    "    #   * limiting to 24-hours per hash should help ensure that we do not incorrectly merge multiple users together\n",
    "\n",
    "    # query for a given day\n",
    "    query_date = dt + timedelta(days=i)\n",
    "\n",
    "    # We exclude edits by the following logic:\n",
    "    # (uri_query LIKE '%action=edit%'): desktop wikitext editor\n",
    "    # (uri_query LIKE '%action=visualeditor%'): desktop and mobile visualeditor\n",
    "    # (uri_query LIKE '%&intestactions=edit&intestactionsdetail=full&uiprop=options%'): mobile wikitext editor\n",
    "    # NOTE for mobile wikitext editor: this is actually an API call so theoretically anyone could make it, but \n",
    "    # in practice we see literally 100% of calls to intestactions=edit are mobile wikitext editor and the rest\n",
    "    # of the URL parameters in the string is to ensure no false positives.\n",
    "    \n",
    "    # We also exclude mobile_app users because they are:\n",
    "    # * a small proportion of our pageviews\n",
    "    # * more identifiable as a result (and have unique app IDs in x_analytics_header)\n",
    "    # * I have not figured out what their edit clauses look like\n",
    "    \n",
    "    # We exclude a blacklist of countries (wmf.geoeditors_blacklist_country)\n",
    "    # with the exception of Iran and Russia given their research relevance for Covid-19\n",
    "    \n",
    "    # We exclude users with > 500 pageviews in a day because anecdotally these are split between:\n",
    "    # * serious power users\n",
    "    # * a bunch of different people all using the same proxy (e.g., Google Weblight)\n",
    "    # * unidentified bots\n",
    "    \n",
    "    # We take 1% of all users and 50% of users who viewed a Covid-19-related page in their session:\n",
    "    # we randomly sample individual users from a day's worth of data to reduce the likelihood of any one\n",
    "    # person showing up consistently in each day's sample. This is done by taking a userhash (before sessionization)\n",
    "    # and converting it to decimal and then sampling based on the final digits of that resulting number.\n",
    "    # For example, a sha256 userhash might be '0d03031932aa1d96d54327ea4754b393a62feb61585fef6cf06daee9cf848b27'\n",
    "    # In decimal, this would be: 5885388957350101327411052618704501086212173454334169283944732915681982712615\n",
    "    # If we take the final four digits of that (2615), we can determine whether the user is part of a 1% sample by\n",
    "    # testing whether that number is less than 100. In this case, no, but if the userhashes are randomly distributed\n",
    "    # (which they should be), then 1% of the final four digits should be between 0000 and 0099 and the other 99% should\n",
    "    # be from 0100 to 9999. This can be adjusted to e.g., 50% by checking whether the digits are less than 5000.\n",
    "    \n",
    "    # After all the pageviews associated with the sampled users are collected, we order them by timestamp and\n",
    "    # separate them into 1 or more sessions, where a session is defined as >1 hour between consecutive pageviews.\n",
    "    # Userhashes are then reassigned so it's not easy to determine whether a given user had multiple sessions in\n",
    "    # a day or not.\n",
    "    \n",
    "    # Timestamps in the final dataset are reported only as year, month, day, hour. To preserve the research value\n",
    "    # of knowing time between pageviews while still reducing the privacy risk that fine-grained timestamps pose,\n",
    "    # we also compute the number of minutes (integer) between each pageview and indicate the order of pageviews.\n",
    "    \n",
    "    query = \"\"\"\n",
    "    INSERT OVERWRITE TABLE {0}\n",
    "           PARTITION(year={1}, month={2}, day={3})\n",
    "    WITH wikipedia_projects AS (\n",
    "        SELECT DISTINCT hostname,\n",
    "               dbname\n",
    "          FROM wmf_raw.mediawiki_project_namespace_map\n",
    "         WHERE snapshot = '2020-01'\n",
    "               AND hostname LIKE '%wikipedia%'\n",
    "        ),\n",
    "    wikidata_ids AS (\n",
    "        SELECT SUBSTR(p.hostname, 0, length(p.hostname)-4) AS project,\n",
    "               wd.page_id AS page_id,\n",
    "               wd.item_id AS item_id\n",
    "          FROM wmf.wikidata_item_page_link wd\n",
    "          LEFT JOIN wikipedia_projects p\n",
    "               ON (wd.wiki_db = p.dbname)\n",
    "         WHERE snapshot = '2020-03-02'\n",
    "               AND page_namespace = 0\n",
    "        ),\n",
    "    blacklisted_countries AS (\n",
    "        SELECT DISTINCT country\n",
    "          FROM wmf.geoeditors_blacklist_country\n",
    "         WHERE country NOT IN ('Russia', 'Iran')\n",
    "        ),\n",
    "    users_to_keep AS (\n",
    "        SELECT sha2(CONCAT(user_agent, client_ip, '{4}'), 256) AS user,\n",
    "               w.geocoded_data['country'] AS country,\n",
    "               MAX(CAST(COALESCE(cvd.page_id, -1) > 0 AS int)) AS is_covid,\n",
    "               COUNT(1) AS num_pviews\n",
    "          FROM wmf.webrequest w\n",
    "          LEFT JOIN covid_pages cvd\n",
    "               ON (w.page_id = cvd.page_id AND pageview_info['project'] = cvd.project)\n",
    "          LEFT ANTI JOIN blacklisted_countries bc\n",
    "               ON (w.geocoded_data['country'] = country)\n",
    "         WHERE webrequest_source = 'text'\n",
    "               AND normalized_host.project_family = 'wikipedia'\n",
    "               AND year = {1} AND month = {2} AND day = {3}\n",
    "               AND agent_type = 'user'\n",
    "               AND access_method <> 'mobile app'\n",
    "               AND ((is_pageview AND (namespace_id = 0 OR namespace_id = 1))\n",
    "                    OR (uri_query LIKE '%action=edit%' OR uri_query LIKE '%action=visualeditor%' OR uri_query LIKE '%&intestactions=edit&intestactionsdetail=full&uiprop=options%'))\n",
    "             GROUP BY sha2(CONCAT(user_agent, client_ip, '{4}'), 256), w.geocoded_data['country']\n",
    "            HAVING COUNT(1) < 500\n",
    "                   AND MAX(CAST(NOT is_pageview as int)) = 0\n",
    "                   AND ((is_covid > 0 AND CONV(SUBSTR(user, 49), 16, 10) / 18446744073709551615 < 0.5)\n",
    "                        OR CONV(SUBSTR(user, 49), 16, 10) / 18446744073709551615 < 0.01)\n",
    "        ),\n",
    "    countries_to_drop AS (\n",
    "           SELECT DISTINCT(country)\n",
    "             FROM (\n",
    "               SELECT country\n",
    "                 FROM users_to_keep\n",
    "                GROUP BY country, is_covid\n",
    "               HAVING COUNT(DISTINCT(user)) < 500\n",
    "                UNION ALL\n",
    "               SELECT country\n",
    "                 FROM users_to_keep\n",
    "                GROUP BY country\n",
    "               HAVING SUM(num_pviews * is_covid) > (0.90 * SUM(num_pviews))\n",
    "                  ) c\n",
    "        ),\n",
    "       sessions AS (\n",
    "           SELECT sha2(CONCAT(user_agent, client_ip, '{4}'), 256) AS user,\n",
    "                  geocoded_data['continent'] AS continent,\n",
    "                  geocoded_data['country'] AS country,\n",
    "                  geocoded_data['subdivision'] AS subdivision,\n",
    "                  geocoded_data['timezone'] AS timezone,\n",
    "                  ts, hour,\n",
    "                  pageview_info['project'] as project,\n",
    "                  w.namespace_id as namespace_id,\n",
    "                  COALESCE(wd.item_id, 'None') as qid,\n",
    "                  w.page_id AS page_id,\n",
    "                  pageview_info['page_title'] AS title,\n",
    "                  CAST(COALESCE(cvd.page_id, -1) > 0 AS int) > 0 AS is_covid,\n",
    "                  PARSE_URL(referer, 'HOST') as referer,\n",
    "                  referer_class,\n",
    "                  access_method,\n",
    "                  COALESCE(x_analytics_map['WMF-Last-Access'], 'NULL') AS last_access,\n",
    "                  CASE\n",
    "                    WHEN (unix_timestamp(ts) - COALESCE(LAG(unix_timestamp(ts)) OVER (PARTITION BY sha2(CONCAT(user_agent, client_ip, '{4}'), 256) ORDER BY ts), 0)) >= {6}\n",
    "                      THEN 1 ELSE 0\n",
    "                  END AS new_session\n",
    "             FROM wmf.webrequest w\n",
    "            INNER JOIN users_to_keep u\n",
    "                  ON (u.user = sha2(CONCAT(user_agent, client_ip, '{4}'), 256))\n",
    "             LEFT ANTI JOIN countries_to_drop c\n",
    "                  ON (c.country = w.geocoded_data['country'])\n",
    "             LEFT JOIN covid_pages cvd\n",
    "                  ON (w.pageview_info['project'] = cvd.project AND w.page_id = cvd.page_id)\n",
    "             LEFT JOIN wikidata_ids wd\n",
    "                  ON (w.pageview_info['project'] = wd.project AND w.page_id = wd.page_id)\n",
    "            WHERE webrequest_source = 'text'\n",
    "                  AND normalized_host.project_family = 'wikipedia'\n",
    "                  AND year = {1} AND month = {2} AND day = {3}\n",
    "                  AND agent_type = 'user'\n",
    "                  AND access_method <> 'mobile app'\n",
    "                  AND is_pageview\n",
    "                  AND (namespace_id = 0 OR namespace_id = 1)\n",
    "        ),\n",
    "      anonymized_sessions AS (\n",
    "          SELECT sha2(CONCAT(user, '{5}', SUM(new_session) OVER (PARTITION BY user ORDER BY ts)), 256) AS session_hash,\n",
    "                 continent, country, subdivision, timezone,\n",
    "                 year, ts,\n",
    "                 project, qid, namespace_id, page_id, title, is_covid,\n",
    "                 referer, referer_class, access_method, last_access\n",
    "            FROM sessions s\n",
    "        )           \n",
    "       SELECT session_hash,\n",
    "              continent, country, subdivision, timezone,\n",
    "              hour,\n",
    "              project, qid, namespace_id, page_id, title, is_covid,\n",
    "              referer, referer_class, access_method, last_access,\n",
    "              IF(sec_btw_pvs >= {6}, 0, ROUND(sec_btw_pvs / 60)) AS min_btw_pvs,\n",
    "              ROW_NUMBER() OVER w as session_sequence,\n",
    "              ROUND(COALESCE(unix_timestamp(ts) - LAG(unix_timestamp(ts)) OVER w, 0) / 60) as min_btw_pvs\n",
    "         FROM anonymized_sessions\n",
    "       WINDOW w AS (PARTITION BY session_hash ORDER BY ts)\n",
    "     \"\"\".format(table_name, query_date.year, query_date.month, query_date.day,\n",
    "                salt_one, salt_two, session_length, ip_match)    \n",
    "        \n",
    "    if print_for_hive:\n",
    "        print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "    else:\n",
    "        print(re.sub(r'SUBSTR\\(ip, -1, 1\\) = [0-9]',\n",
    "                     \"SUBSTR(ip, -1, 1) = <#>\",\n",
    "                     re.sub(r\"CONCAT\\(user_agent, client_ip, '[a-zA-Z0-9]+'\\)\",\n",
    "                            \"CONCAT(user_agent, client_ip, '<SALT-1>')\",\n",
    "                            re.sub(r\"CONCAT\\(user, '[a-zA-Z0-9]+', SUM\\(\",\n",
    "                                   \"CONCAT(user, '<SALT-2>', SUM(\",\n",
    "                                   query))))\n",
    "    \n",
    "    if do_execute:\n",
    "        spark.sql(query)\n",
    "\n",
    "# clear salt and ip_match so not accidentally retained\n",
    "ip_match = None\n",
    "salt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Country statistics (TODO: update to be day partition in static table name)\n",
    "data_table = 'isaacj.sample_sessions_20200327'\n",
    "spark.sql(\"\"\"\n",
    "    SELECT country,\n",
    "           SUM(is_covid) as covid_sessions,\n",
    "           COUNT(1) - SUM(is_covid) as non_covid_sessions,\n",
    "           SUM(num_pviews * is_covid) as covid_pviews,\n",
    "           SUM(num_pviews) - SUM(num_pviews * is_covid) as noncovid_pviews\n",
    "      FROM (SELECT session_hash,\n",
    "                   country,\n",
    "                   CAST(MAX(is_covid) as int) as is_covid,\n",
    "                   COUNT(1) as num_pviews\n",
    "              FROM {0}\n",
    "             GROUP BY session_hash, country\n",
    "           ) s\n",
    "     GROUP BY country\n",
    "     ORDER BY covid_pviews DESC\n",
    "     LIMIT 1000\n",
    "     \"\"\".format(data_table)).show(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if need to push the data to a TSV for further processing (not currently used)\n",
    "write_to_tsv = False\n",
    "if write_to_tsv:\n",
    "    for i in range(num_days):\n",
    "        query_date = dt + timedelta(days=i)\n",
    "\n",
    "        day_data = spark.sql('SELECT * FROM {0} WHERE year = {1} and month = {2} and day = {3}'.format(\n",
    "            table_name, query_date.year, query_date.month, query_date.day))\n",
    "        day_data.show(10)\n",
    "        day_data.coalesce(1).write.csv(path=\"/user/isaacj/covid19/wr_{0}\".format(t_date), compression=\"gzip\", header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate intermediate data for a given country to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_for_hive = False\n",
    "do_execute = False\n",
    "country = 'Ghana'\n",
    "\n",
    "# salt for UA/IP hash\n",
    "salt = ''.join(choice(string.ascii_lowercase + string.ascii_uppercase + string.digits) for _ in range(randint(8,16)))\n",
    "\n",
    "# matching by last digit of IP is ~random means of sampling devices (no obvious skew)\n",
    "# Due to IPv6, IP addresses can actually end in [0-9] or [a-f]\n",
    "# There are still many more IPv4 addresses than IPv6 though, so using a [0-9] gives you ~5x more data than a letter\n",
    "# North America has a slightly higher proportion of IPv6 so they are undersampled slightly through this method\n",
    "# consider adding to innermost where clause: AND SUBSTR(ip, -1, 1) = {4}\n",
    "ip_match = randint(0, 9)\n",
    "        \n",
    "# query for a given day\n",
    "query_date = dt\n",
    "    \n",
    "query = \"\"\"\n",
    "    SELECT sha2(CONCAT(user_agent, client_ip, '{0}'), 256) AS user,\n",
    "           MAX(CAST(COALESCE(cvd.page_id, -1) > 0 AS int)) AS is_covid,\n",
    "           MAX(CAST(NOT is_pageview as int)) AS editor,\n",
    "           COUNT(1) AS num_pviews,\n",
    "           COUNT(1) < 500 AS power_user,\n",
    "           CONV(sha2(SUBSTR(CONCAT(user_agent, client_ip, '{0}'), 256), 49), 16, 10) / 18446744073709551615 as rand_sample\n",
    "      FROM wmf.webrequest w\n",
    "      LEFT JOIN covid_pages cvd\n",
    "           ON (w.page_id = cvd.page_id AND pageview_info['project'] = cvd.project)\n",
    "     WHERE webrequest_source = 'text'\n",
    "           AND normalized_host.project_family = 'wikipedia'\n",
    "           AND geocoded_data['country'] = '{5}'\n",
    "           AND year = {1} AND month = {2} AND day = {3}\n",
    "           AND agent_type = 'user'\n",
    "           AND access_method <> 'mobile app'\n",
    "           AND ((is_pageview AND (namespace_id = 0 OR namespace_id = 1))\n",
    "                OR (uri_query LIKE '%action=edit%' OR uri_query LIKE '%action=visualeditor%' OR uri_query LIKE '%&intestactions=edit&intestactionsdetail=full&uiprop=options%'))\n",
    "         GROUP BY sha2(CONCAT(user_agent, client_ip, '{0}'), 256)\n",
    " \"\"\".format(salt, query_date.year, query_date.month, query_date.day, ip_match, country)    \n",
    "\n",
    "table = 'isaacj.{0}_sessions_{1}'.format(country, datetime.strftime(query_date, \"%Y%m%d\"))\n",
    "query = 'CREATE TABLE {0} AS {1}'.format(table, query)\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(re.sub(r'SUBSTR\\(ip, -1, 1\\) = [0-9]', \"SUBSTR(ip, -1, 1) = <#>\",\n",
    "                 re.sub(r\"CONCAT\\(user_agent, client_ip, '[a-zA-Z0-9]+'\\)\", '<UH CLAUSE>', query)))\n",
    "\n",
    "if do_execute:\n",
    "    spark.sql(query)\n",
    "\n",
    "# clear salt and ip_match so not accidentally retained\n",
    "ip_match = None\n",
    "salt = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN",
   "language": "python",
   "name": "spark_yarn_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
